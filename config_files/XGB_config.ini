[RUN]
# Name
name = XGB_SMILES2SMILES
# Path to the results directory
output_dir = models
# Path to the data file
data_path = data/d2_ECFP_100nM.parquet
# Path to the trained RNN model weights
model_path = models/smiles2smiles_anneal_beta_0.1/epoch_800.pt
# If true, GPU will be used for encoding the data
use_cuda = False
# Search a grid of hyperparameters to find the best model. If false, the model will be trained with the hyperparameters
# specified below (recommended, but may take a long time on weak machines)
optimize_hyperparameters = True

[XGB]

# The number of threads to use for XGBoost
nthread = 8
# Learning rate. Usually a float between 0.01 and 0.3
learning_rate = 0.1
# The number of trees to build. Usually an integer between 100 and 1000
n_estimators = 100
# The maximum depth of the tree. Usually an integer between 3 and 10
max_depth = 3
# The minimum sum of instance weight needed in a child
min_child_weight = 1
# Minimum loss reduction required to make a further partition
gamma = 0.1
# Subsample ratio of the training instances. Prevents overfitting. Usually a float between 0.5 and 1
subsample = 0.8